{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 \n",
    "If you suspect that the server is trying to limit the response, here are some strategies to help you overcome this issue:\n",
    "\n",
    "Implement a delay between requests: Introduce a delay between requests to slow down the scraping process. This can help avoid overwhelming the server and reduce the likelihood of being rate-limited. You can use the time module in Python to introduce a delay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ...\n",
    "\n",
    "for page in range(1, 101):\n",
    "    # ...\n",
    "    response = httpx.get(url, headers=headers)\n",
    "    # ...\n",
    "    time.sleep(1)  # Introduce a 1-second delay between requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 \n",
    "Use a rotating user agent: Some websites block requests from a single user agent to prevent scraping. You can use a rotating user agent to make your requests appear more diverse. You can use libraries like fake-useragent or user-agents to rotate your user agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent()\n",
    "\n",
    "for page in range(1, 101):\n",
    "    headers = {\"User-Agent\": ua.random}\n",
    "    response = httpx.get(url, headers=headers)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "Use a proxy server: If the server is blocking your IP address, you can use a proxy server to mask your IP address. You can use libraries like requests-proxy or httpx-proxy to configure a proxy server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "proxy_url = \"http://my-proxy-server:8080\"\n",
    "proxies = {\"http\": proxy_url, \"https\": proxy_url}\n",
    "\n",
    "for page in range(1, 101):\n",
    "    response = httpx.get(url, headers=headers, proxies=proxies)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "Handle rate limiting errors: If the server returns a rate limiting error (e.g., 429 Too Many Requests), you can catch the error and retry the request after a certain delay. You can use the httpx library's built-in support for retrying requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "for page in range(1, 101):\n",
    "    try:\n",
    "        response = httpx.get(url, headers=headers)\n",
    "    except httpx.RequestError as e:\n",
    "        if e.response.status_code == 429:\n",
    "            print(\"Rate limit exceeded. Retrying in 30 seconds...\")\n",
    "            time.sleep(30)\n",
    "            response = httpx.get(url, headers=headers)\n",
    "        else:\n",
    "            raise\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "Respect the website's robots.txt file: Make sure you're respecting the website's robots.txt file, which may specify crawl delays or other restrictions.\n",
    "## 6\n",
    "Consider using a scraping framework: If you're building a large-scale scraper, consider using a scraping framework like Scrapy, which has built-in support for handling rate limiting, proxies, and other scraping-related issues."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
